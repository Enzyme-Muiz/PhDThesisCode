{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##differentiate MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the parent directory\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(f\"{parent_dir}\\\\UsedSourceCodes\\\\wishbone_folder\\\\wishbone\\\\src\")\n",
    "sys.path.append(f\"{parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    }
   ],
   "source": [
    "import wishbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wishbone\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.optimizers import adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from UsedSourceCodes.loss_functions_for_ml_batch_effect import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DEFINE MODELS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(x, y, size, steps_per_epoch):\n",
    "    for i in range(steps_per_epoch):\n",
    "        batch_x= x.sample(n = size, replace=True)\n",
    "        batch_y= y.sample(n = size, replace=True)\n",
    "        #yield (np.array(batch_x), np.concatenate((np.array(batch_y), np.array(batch_x)), axis =0))   ###why concat\n",
    "        yield (np.array(batch_x), np.array(batch_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.autograph.experimental.do_not_convert\n",
    "def model_that_corrects_batch_effect(unchanged_df_to_be_used_for_correction_from_batch,\n",
    "                                     to_be_changed_batch_df,\n",
    "                                     depth_of_the_model,\n",
    "                                     epochs,\n",
    "                                     steps_per_epoch,\n",
    "                                     cells_per_batch,\n",
    "                                     val_unchanged_df_to_be_used_for_correction_from_batch,\n",
    "                                     val_to_be_changed_batch_df,\n",
    "                                     first_activation,\n",
    "                                     internal_activation\n",
    "                                    ):\n",
    "    l1_lambda = 0.01\n",
    "    l2_lambda = 0.01\n",
    "    \n",
    "    input_shape = to_be_changed_batch_df.shape[1]\n",
    "    inp = Input((input_shape,), name=\"input1\")\n",
    "    x = Dense(input_shape, activation=first_activation, kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(inp)\n",
    "    #x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    n = input_shape\n",
    "    for i in range(depth_of_the_model):\n",
    "        n = n + 5\n",
    "        x = Dense(n, activation= internal_activation, kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(x)\n",
    "    output1 = Dense(input_shape, activation='relu', name=\"output11\", kernel_regularizer=regularizers.l1(l1_lambda), kernel_initializer='identity')(x)    \n",
    "    adamoptimizer = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "    new_model = Model(inputs=inp, outputs=[output1, output1])\n",
    "    new_model.compile(\n",
    "                     loss = [mmd2_1,\n",
    "                              #wasserstein, \n",
    "                              d_loss_1, \n",
    "                              distance_matrix,\n",
    "                              #KLDivergence()\n",
    "                             ],\n",
    "                     loss_weights= [0.50,\n",
    "                                    0.30,\n",
    "                                    0.20,\n",
    "                                    #0.15, \n",
    "                                    #0.15\n",
    "                                   ],\n",
    "                \n",
    "                      #loss= outer_sinkhorn_loss(n = cells_per_batch,niter = epochs, epsilon = 0.001, p=2), \n",
    "                       #loss = get_loss,\n",
    "                      optimizer= adamoptimizer)\n",
    "    es = EarlyStopping(monitor= \"val_loss\", mode='min', verbose=2, patience=10)\n",
    "    if val_unchanged_df_to_be_used_for_correction_from_batch.empty:\n",
    "                \n",
    "        hist = new_model.fit_generator(generator(to_be_changed_batch_df, unchanged_df_to_be_used_for_correction_from_batch, \n",
    "                                       cells_per_batch, steps_per_epoch*epochs), \n",
    "                             steps_per_epoch = steps_per_epoch, \n",
    "                             epochs= epochs, \n",
    "                            )\n",
    "    elif not val_unchanged_df_to_be_used_for_correction_from_batch.empty:\n",
    "        hist = new_model.fit_generator(generator(to_be_changed_batch_df, unchanged_df_to_be_used_for_correction_from_batch, cells_per_batch, steps_per_epoch*epochs), \n",
    "                             validation_data = generator(val_to_be_changed_batch_df, val_unchanged_df_to_be_used_for_correction_from_batch, cells_per_batch, ((steps_per_epoch+5)*(epochs+5))),\n",
    "                             validation_steps = steps_per_epoch,\n",
    "                             steps_per_epoch = steps_per_epoch, \n",
    "                             epochs= epochs, \n",
    "                             \n",
    "                             callbacks=[es]\n",
    "                            )\n",
    "        \n",
    "    return([new_model, hist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "###generator\n",
    "def generator2(x, size, steps_per_epoch):\n",
    "    for i in range(steps_per_epoch):\n",
    "        batch_x= x.sample(n = size, replace=True)\n",
    "        yield np.array(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "###model\n",
    "def model_2():\n",
    "    l1_lambda = 0.01\n",
    "    l2_lambda = 0.01\n",
    "    depth_of_the_model = 0\n",
    "    input_shape = 22  ## hardcoded\n",
    "    inp = Input((input_shape,), name=\"input1\")\n",
    "    x = Dense(input_shape, activation='linear', kernel_regularizer=regularizers.l1(l1_lambda))(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(rate = 0.2)(x)\n",
    "    n = input_shape\n",
    "    for i in range(depth_of_the_model):\n",
    "        n = n + 5\n",
    "        x = Dense(n, activation='elu', kernel_regularizer=regularizers.l1(l1_lambda))(x)\n",
    "    output1 = Dense(input_shape, activation='relu', name=\"output11\", kernel_regularizer=regularizers.l1(l1_lambda))(x)    \n",
    "    adamoptimizer = tf.keras.optimizers.Adam(learning_rate= 0.01)\n",
    "    new_model = Model(inputs=inp, outputs=output1 )\n",
    "    new_model.compile(\n",
    "        loss = [mmd2, d_loss,],\n",
    "        loss_weights= [0.50, 0.50,],\n",
    "        #loss = mmd2, \n",
    "        optimizer= adamoptimizer)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajim\\Desktop\\PHDcodes\n"
     ]
    }
   ],
   "source": [
    "parent_parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
    "print(parent_parent_dir)\n",
    "sys.path.append(f\"{parent_parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st ag3650_M.fcs\n",
      "1st ai3661_M.fcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\fcsparser\\api.py:326: UserWarning: The first two characters were:\n",
      " \\$. The last two characters were: \\\u0000\n",
      "Parser expects the same delimiter character in beginning and end of TEXT segment. This file may be parsed incorrectly!\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\rajim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\fcsparser\\api.py:326: UserWarning: The first two characters were:\n",
      " \\$. The last two characters were: \\\u0000\n",
      "Parser expects the same delimiter character in beginning and end of TEXT segment. This file may be parsed incorrectly!\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st ah3656_UM.fcs\n",
      "1st aj3663_M.fcs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rajim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\fcsparser\\api.py:326: UserWarning: The first two characters were:\n",
      " \\$. The last two characters were: \\\u0000\n",
      "Parser expects the same delimiter character in beginning and end of TEXT segment. This file may be parsed incorrectly!\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\rajim\\AppData\\Local\\anaconda3\\Lib\\site-packages\\fcsparser\\api.py:326: UserWarning: The first two characters were:\n",
      " \\$. The last two characters were: \\\u0000\n",
      "Parser expects the same delimiter character in beginning and end of TEXT segment. This file may be parsed incorrectly!\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "path = f\"{parent_parent_dir}/data/experiment_UMvsM\"\n",
    "list_files = os.listdir(path)\n",
    "\n",
    "n1 = 6\n",
    "new1 = list_files[n1]\n",
    "print(new1)\n",
    "new1 = path+\"/\" + new1\n",
    "scdata1 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new1),\n",
    "            cofactor= None)\n",
    "\n",
    "n2= 8\n",
    "new2 = list_files[n2]\n",
    "print(new2)\n",
    "new2 = path+\"/\" + new2\n",
    "scdata2 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new2),\n",
    "            cofactor= None)\n",
    "\n",
    "new3 = list_files[n1+1]\n",
    "print(new3)\n",
    "new3 = path+\"/\" + new3\n",
    "scdata3 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new3),\n",
    "            cofactor= None)\n",
    "\n",
    "new4 = list_files[n2+1]\n",
    "print(new4)\n",
    "new4 = path+\"/\" + new4\n",
    "scdata4 = wishbone.wb.SCData.from_fcs(os.path.expanduser(new4),\n",
    "            cofactor= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnstouse = ['141Pr_CD196', '142Nd_CD19', '143Nd_CD5', '144Nd_CD38', '145Nd',\n",
    "       '146Nd_IgD', '147Sm_CD11c', '148Nd_CD16', '149Sm_CCR4', '150Nd_CD43',\n",
    "       '151Eu_CD69', '152Sm_CD21', '153Eu_CXCR5', '154Sm_CD62L',\n",
    "       '155Gd_CD45RA', '158Gd_CD27', '159Tb_CD22', '160Gd_CD14', '162Dy',\n",
    "       '163Dy_CXCR3', '164Dy_CD23', '165Ho_CD45_BC', '166Er_CD24',\n",
    "       '167Er_CCR7', '168Er_CD8a', '169Tm_CD45_BC', '170Er_CD3', '171Yb_CD20',\n",
    "       '172Yb_IgM', '173Yb_HLA-DR', '174Yb_CD49d', '175Lu_CXCR4', '176Yb_CD56']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "scdata1.data = scdata1.data[columnstouse]\n",
    "scdata2.data = scdata2.data[columnstouse]\n",
    "scdata3.data = scdata3.data[columnstouse]\n",
    "scdata4.data = scdata4.data[columnstouse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(scdata1.data)\n",
    "\n",
    "scaler5 = StandardScaler()\n",
    "scaler5.fit(scdata3.data)\n",
    "\n",
    "column_names = scdata1.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SCData' object has no attribute 'normalize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[135], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scdata1\u001b[38;5;241m.\u001b[39mnormalize()\n\u001b[0;32m      2\u001b[0m scdata2\u001b[38;5;241m.\u001b[39mnormalize()\n\u001b[0;32m      3\u001b[0m scdata3\u001b[38;5;241m.\u001b[39mnormalize()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SCData' object has no attribute 'normalize'"
     ]
    }
   ],
   "source": [
    "scdata1.normalize()\n",
    "scdata2.normalize()\n",
    "scdata3.normalize()\n",
    "scdata4.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30, metric=\"euclidean\", random_state=42)\n",
    "X_embedded = tsne.fit(np.array(scdata1.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m scdata1\u001b[38;5;241m.\u001b[39mrun_tsne()\n",
      "File \u001b[1;32m~\\Desktop\\PHDcodes\\MyPhDCodes\\UsedSourceCodes\\wishbone_folder\\wishbone\\src\\wishbone\\wb.py:425\u001b[0m, in \u001b[0;36mSCData.run_tsne\u001b[1;34m(self, n_components, perplexity, rand_seed)\u001b[0m\n\u001b[0;32m    423\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m perplexity_limit\n\u001b[0;32m    424\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 425\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m tsne\u001b[38;5;241m.\u001b[39mfit(data)\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtsne \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_embedded,\n\u001b[0;32m    427\u001b[0m                  index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openTSNE\\tsne.py:1247\u001b[0m, in \u001b[0;36mTSNE.fit\u001b[1;34m(self, X, affinities, initialization)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m, \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1247\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_initial(X, affinities, initialization)\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m     \u001b[38;5;66;03m# Early exaggeration with lower momentum to allow points to find more\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;66;03m# easily move around and find their neighbors\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m     embedding\u001b[38;5;241m.\u001b[39moptimize(\n\u001b[0;32m   1253\u001b[0m         n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_exaggeration_iter,\n\u001b[0;32m   1254\u001b[0m         exaggeration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_exaggeration,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1257\u001b[0m         propagate_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1258\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openTSNE\\tsne.py:1325\u001b[0m, in \u001b[0;36mTSNE.prepare_initial\u001b[1;34m(self, X, affinities, initialization)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;66;03m# If precomputed affinites are given, use those, otherwise proceed with\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;66;03m# standard perpelxity-based affinites\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m affinities \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m     affinities \u001b[38;5;241m=\u001b[39m MultiscaleMixture(\n\u001b[0;32m   1326\u001b[0m         X,\n\u001b[0;32m   1327\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity,\n\u001b[0;32m   1328\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneighbors,\n\u001b[0;32m   1329\u001b[0m         metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   1330\u001b[0m         metric_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_params,\n\u001b[0;32m   1331\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m   1332\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m   1333\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m   1334\u001b[0m     )\n\u001b[0;32m   1335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1336\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(affinities, Affinities):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openTSNE\\affinity.py:817\u001b[0m, in \u001b[0;36mMultiscaleMixture.__init__\u001b[1;34m(self, data, perplexities, method, metric, metric_params, symmetrize, n_jobs, random_state, verbose, knn_index)\u001b[0m\n\u001b[0;32m    814\u001b[0m     effective_perplexities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_perplexities(perplexities, n_samples)\n\u001b[0;32m    815\u001b[0m     log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNN index provided. Ignoring KNN-related parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 817\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__neighbors, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_index\u001b[38;5;241m.\u001b[39mbuild()\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mTimer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating affinity matrix...\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose):\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_P(\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__neighbors,\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__distances,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    825\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    826\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openTSNE\\nearest_neighbors.py:253\u001b[0m, in \u001b[0;36mAnnoy.build\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mset_seed(random_state\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax))\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39madd_item(i, data[i])\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Number of trees. FIt-SNE uses 50 by default.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m50\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "scdata1.run_tsne()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###data\n",
    "source_dataset = scdata1.data\n",
    "source_dataset1 = scdata2.data\n",
    "target_dataset = scdata3.data\n",
    "target_dataset2 = scdata4.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = scdata1.data\n",
    "B = scdata2.data\n",
    "C = scdata3.data\n",
    "D = scdata4.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the base mmd value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_value_for_anchor = MMD(A, B, 1)\n",
    "base_value_for_anchor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_value_for_validation = MMD(C, D, 1)\n",
    "base_value_for_validation.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "number_of_cell =2000\n",
    "source_iterator = generator2(source_dataset, number_of_cell, num_epochs)\n",
    "source_iterator1 = generator2(source_dataset1, number_of_cell, num_epochs)\n",
    "target_iterator = generator2(target_dataset, number_of_cell, num_epochs)\n",
    "target_iterator1 = generator2(target_dataset, number_of_cell, num_epochs)\n",
    "# Iterate over epochs\n",
    "new_model = model_2()\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}:\")\n",
    "    source_batch = next(source_iterator)\n",
    "    source_batch1 = next(source_iterator1)\n",
    "    loss = new_model.train_on_batch(source_batch, source_batch1)  # Update the model with the source dataset\n",
    "    print(f'Custom Loss: {loss}')\n",
    "    \n",
    "    target_batch = next(target_iterator)\n",
    "    target_batch1 = next(target_iterator1)\n",
    "    loss = new_model.train_on_batch(target_batch, target_batch1)  # Update the model with the target dataset\n",
    "    print(f'Custom Loss: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cells_in_each_iteration = 500\n",
    "first = f\"Batch{N1}\"    ###blue\n",
    "second = f\"Batch{N2}\"   ###red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "B= scdata2.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "B = B.reset_index(drop=True)\n",
    "AB_merged = A.append(B, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(new_model.predict(B))\n",
    "res.columns = B.columns\n",
    "\n",
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "\n",
    "AB_merged = A.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = C.reset_index(drop=True)\n",
    "D= scdata4.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "D = D.reset_index(drop=True)\n",
    "CD_merged = C.append(D, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(new_model.predict(D))\n",
    "res.columns = D.columns\n",
    "\n",
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = A.reset_index(drop=True)\n",
    "\n",
    "CD_merged = C.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "###\n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_to_use = 1500\n",
    "model = model_that_corrects_batch_effect(A, B, 1, 10,5, cells_to_use, C, D,\n",
    "                                         \"linear\",\n",
    "                                         \"linear\"\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist= model[1]\n",
    "model = model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first_one = hist.history[\"loss\"]\n",
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history[\"loss\"], c = \"b\", label = \"training\")\n",
    "plt.plot([x for x in hist.history[\"val_loss\"]], c = \"r\", label =\"validation\")\n",
    "#plt.plot(first_one, c = \"r\", label =\"validation\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f\"Training and validation Losses \\nof model for correcting batch effect \\n between batches {N1} and {N2}\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Plot pca before correction of two batches</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N1 = int((n1/2)+1)\n",
    "N2 = int((n2/2)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cells_in_each_iteration = 500\n",
    "first = f\"Batch{N1}\"    ###blue\n",
    "second = f\"Batch{N2}\"   ###red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "B= scdata2.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "B = B.reset_index(drop=True)\n",
    "AB_merged = A.append(B, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.predict(B)[0])\n",
    "res.columns = B.columns\n",
    "\n",
    "A= scdata1.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "A = A.reset_index(drop=True)\n",
    "\n",
    "AB_merged = A.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(AB_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction anchor \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = C.reset_index(drop=True)\n",
    "D= scdata4.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "D = D.reset_index(drop=True)\n",
    "CD_merged = C.append(D, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} before correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(model.predict(D)[0])\n",
    "res.columns = D.columns\n",
    "\n",
    "C= scdata3.data.sample(n = number_of_cells_in_each_iteration, replace=True)\n",
    "C = A.reset_index(drop=True)\n",
    "\n",
    "CD_merged = C.append(res, ignore_index=True)\n",
    "colours = [\"b\"]*number_of_cells_in_each_iteration + [\"r\"]*number_of_cells_in_each_iteration\n",
    "pca = PCA()\n",
    "Xt = pca.fit_transform(CD_merged)\n",
    "#plot = plt.scatter(Xt[:,0], Xt[:,1], c = colours)\n",
    "plt.scatter(Xt[:number_of_cells_in_each_iteration,0], Xt[:number_of_cells_in_each_iteration,1], c = colours[:number_of_cells_in_each_iteration], label = first)\n",
    "plt.scatter(Xt[number_of_cells_in_each_iteration+1:,0], Xt[number_of_cells_in_each_iteration+1:,1], c = colours[number_of_cells_in_each_iteration+1:], label = second)\n",
    "plt.title(f\"{first} vs {second} after correction validation \\nsamples\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
